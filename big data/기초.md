# Clustering

- Clustering: 데이터를 유사도에 따라 k개의 그룹으로 나누는 것
  - 추천 시스템을 만드는데 주로 사용된다.



- Clustering이 잘 되었는지 판별하는 방법

  - n개의 데이터가 있을 때 모든 데이터를 Clustering하는 경우의 수는 2<sup>n</sup>개 이다.

  - Clustering을 할 수 있는 경우의 수는 데이터의 수에 따라 증가하고 빅데이터의 경우 셀 수 없을 만큼 만은 경우의 수가 존재하게 된다. 따라서 어떤 Clustering결과가 가장 유의미한지 판별이 가능해야 한다.

  - 판별 방법에는 여러 가지가 있는데 아래의 수식은 그 중 하나이다.
    $$
    \sum_{m=1}^k\sum_{t_{mi}\in{K_m}}^N (C_m-t_{mi})^2
    $$

    - Clustering한 그룹 내의 모든 포인터의 중심과 모든 그룹 내의 개별 포인터의 차이를 제곱해서 더했을 때 그 값이 작을 수록 Clustering이 더 잘됐다고 볼 수 있다.
    - 예를 들어 (1,1), (1,2), (2,1), (5,5), (5,6), (6,5), (7,7)의 7개의 포인터가 있고 이를  A: (1,1), (1,2), (7,7)  / B:  (2,1), (5,5), (5,6), (6,5)로 나누었을 때 A의 중심좌표는 (9/3,10/3) 즉, (3, 3.3)이고 B의 중심 좌표는 (18/4,17/4) 즉, (4.5, 4.2)이다. 이 때 A의 중심좌표인 (3, 3.3)에서 A의 개별 좌표들을 빼서 제곱한 후 모두 더하고, B의 중심좌표인 (4.5, 4.2)에서 B의 개별 좌표들을 빼서 제곱한 후 모두 더하면 결과값(예시의 경우 68.42)이 나온다.
    - A그룹만 예시로 계산을 해보면 A그룹의 중심 좌표는 (9/3, 10/3)이므로 (1-9/3)<sup>2</sup>+(1-10/3)<sup>2</sup>+(1-9/3)<sup>2</sup>+(2-10/3)<sup>2</sup>+(7-9/3)<sup>2</sup>+(7-10/3)<sup>2</sup>을 하면 결과값은 23.75가 나온다.
    - 반면에 A:(1,1), (1,2), (2,1)  /  B:(5,5), (5,6), (6,5),(7,7)로 나눌 경우 6.83이 나오게 된다. 평면도 상에 좌표를 찍어보면 훨씬 Clustering이 잘 되었다는 것을 눈으로 확인할 수 있다.
    - 이번에도 A그룹만 계산을 해보면 A그룹의 중심 좌표는 (4/3,4/3)이므로 (1-4/3)<sup>2</sup>+(1-4/3)<sup>2</sup>+(1-4/3)<sup>2</sup>+(2-4/3)<sup>2</sup>+(2-4/3)<sup>2</sup>+(1-4/3)<sup>2</sup>을 하면 결과값은 5.5가 나온다. 
    - A그룹의 계산 결과만 봤을 때 (1,1), (1,2), (7,7)로 Clustering했을 때의 결과값인 23.75보다 (1,1), (1,2), (2,1)의 결과값인 5.5가 더 작으므로 A그룹만 봤을 때는 클러스트링이 더 잘 되었다고 볼 수 있다(이 경우에는 B그룹도 값이 더 적게 나와 Clustering이 더 잘된 것이 사실이지만 한 그룹의 결과값이 낮다고 해서 전체적으로 Clustering이 더 잘 되었다고 볼 수 있는 것은 아니다)



## Partitional Clustering Algorithms

- 가능한 Clustering의 결과를 열거해보면서 최적화된 결과를 찾아내는 방식



- 실질적으로 방대한 데이터를 전부 뒤지는 것은 불가능하기 때문에 일부만 뒤지는 방식이 사용된다.



- K-Means Clustering Algorithm

  - 방식
    - 랜덤하게 clustering하여 k개 그룹으로 나눈다.
    - 각 그룹의 평균점(mean)을 계산 한다. 
    - 모든 그룹의 개별 포인터 들이 어떤 그룹의 평균점에 가까운지를 계산한다.
    - 더 평균점이 가까운 그룹으로 재할당한다. 
    - 재할당 후 다시 위의 과정을 반복한다.
    - 재할당될 포인터가 없으면 종료된다.

  - 문제점
    - 데이터의 사이즈가 지나치게 작거나 지나치게 크면 사용이 어렵다.
    - 평균점을 기준으로 데이터가 원형으로 분포하는 종류의 데이터일 때에만 사용이 가능하다.
    - outlier가 있을 경우 평균점 계산이 잘못될 수 있다.
  - k-Medoids
    - K-Means Algorithms의 변형
    - k-means 알고리즘이 포인터들의 평균점을 계산하여 가상의 평균점을 중심으로 데이터를 묶는 방식이라면 k-Medoids는 가상의 평균점이 아닌 실제 존재하는 하나의 포인터를 중심점, 혹은 평균점으로 잡고 이 포인터를 중심으로 데이터를 묶는 방식이다.
    - 일반적으로 포인터들이 모여 있는 곳에 있는 포인터 중에서 중심점이 될 포인터를 선택하기 때문에 outlier의 영향을 적게 받는다.



- Hierarchical Clustering

  - 방식(bottom-up 방식)

    - n개의 모든 개별 포인터들이 독립적인 상태로 시작(즉 n개의 클러스터가 존재)
    - 각 개별 포인터와 다른 모든 포인터의 거리를 계산
    - 가장 가까운 거리에 있는 포인터를 병합(한 번 할 때마다 클러스터의 수가 감소), 동일할 경우 가장 가까운 포인터중 아무 포인터나 선택
    - 원하는 클러스터의 수(k)가 될 때 까지 이를 반복

  - 몇 개의 포인터를 가진 서로 다른 클러스터 사이의 거리를 계산하는 방식은 몇 가지가 존재한다. 어떤 방식을 채택하는가에 따라 결과가 달라질 수 있다.

    - Single-link Algorithm(d<sub>min</sub>): 한 클러스터의 모든 포인터들과 다른 클러스터의 모든 포인터들의 거리를 전부 계산한 후 가장 짧은 거리를 거리로 간주

      ![](C:\Users\Cha\Desktop\min.png)

    - Mean-link(d<sub>mean</sub>): 두 클러스터를 병합한 후 병합된 클러스터 내부의 모든 개별 포인터의 거리를 계산하여 그 평균을 거리로 간주

      ![](C:\Users\Cha\Desktop\mean.png)

    - Average-linkd(d<sub>avg</sub>): 한 클러스터의 모든 포인터들과 다른 클러스터의 모든 포인터들의 거리를 전부 계산한 후 그 평균값을 거리로 간주

      ![](C:\Users\Cha\Desktop\average.png)

    - Complete-link Algorithm(d<sub>max</sub>): 한 클러스터의 모든 포인터들과 다른 클러스터의 모든 포인터들의 거리를 전부 계산한 후 가장 긴 거리를 거리로 간주

      ![](C:\Users\Cha\Desktop\max.png)

    - Centroid-link: 각 클러스터의 중심값 사이의 거리를 거리로 간주(빨간 점 I, J는 가상의 평균점)

      ![](C:\Users\Cha\Desktop\centroid.png)



## Density-Based Clustering Algorithms

- DBSCAN Clustering Algorithm
  - 용어
    - Eps: ε(epsilon), 특정 점에서부터의 거리
    - MinPts: Eps 내에 있는 점들의 수
    - Core point: 점 p의 Eps 내에 자기 자신을 포함하여 Minpts 이상의 점이 있으면 점 p는 Core point라고 할 수 있다.
    - neighbourhood: core point인 점 p의 Eps 내에 속하는 점들, neighbourhood이면서 core point일 수도 있으며 neighbourhood이면서 core point가 아닌 점을 border point라 부른다.
    - Border point: Core point가 되지는 못하지만 한 Core point의 Eps내에 있는 점을 Border point라 부른다.
    - Directly-density-reachable:  core point인 p와 p에 속한(neighbourhood인) q는 p로부터 directly-density-reachable하다고 할 수 있다.
    - Density-reachable: core point인 p의  neighbourhood이면서 core point인 r이 있고 q가 r의 neighbourhood라면 p로부터 q 는 density-reachable하다고 할 수 있다(단 p는 q의 neighbourhood가 아니어야 한다).
    - Density-connected: 점 p, q가 있을 때 또 다른 점 o가 있어서 o로부터 p로도 density-reachable하고 q로도 density-reachable하면 density-connected라고 부른다.
    - outlier: core point가 아니면서 다른 core point의 neighbourhood도 아닌(border point도 아닌)점을 outlier라 부른다.
  - 파라미터로 Eps와 MinPts를 필요로 한다.
    - density를 결정하는 파라미터
  -  DBSCAN Clustering Algorithm의 clustering 결과는 두 프로퍼티를 만족해야 한다.
    - Maximality: data에 속한 임의의 쌍 Pi, Pj가 있을때 Pi가 결과상의 임의의 cluseter C에 속하면서 Pj가 Pi로부터 density-reachable하다면 Pj도 Pi와 같은 cluster인 C에 속해야 한다.
    - Connectivity: 만일 Pi, Pj가 같은 cluster인 C에 속한다면 Pi와 Pj는 density-connected여야 한다.
  - 방식
    - 하나의 core point를 찾는다.
    - 해당 core point의 neighbourhood들을 탐색하며 core point인 것을 찾는다.
    - 또 다른 core point가 있을 경우 위 과정을 반복한다.
    - border point만 남았을 경우 중단하며 하나의 cluseter가 완성된다.



## EM Clustering

- 추천 시스템을 구현하는 가장 기초적인 Clustering



- Generative model(생성 모델)
  - 데이터를 무선적으로 생성하는 모델이 존재한다고 가정, 모델은 사람들이 알 수 없으며 데이터를 기반으로 모델(파라미터)을 추측해야 한다. 모집단 추정과 유사
  - 예시
    - 알려진 파라미터: A와 B의 2개의 주머니가 존재한다, 주머니 안에는 빨간색 공과 파란색 공이 존재한다. 한 주머니 당 공은 4개씩 들어 있다.
    - 데이터: 총 9개의 공을 꺼낸 결과 빨간 공 8개와 파란 공 1개가 선택되었다.
    - 데이터와 알려진 파라미터를 기반으로 추측해야 하는 파라미터: A와 B 주머니를 선택할 확률, 한 주머니의 빨간색 공과 파란색 공의 비율
    - `모델1`: A 주머니에는 빨간 공 4개가, B 주머니에는 빨간 공 3개와 파란 공 1개가 있을 것이고 이 때 주머니 선택 단계에서 A 주머니를 선택할 확률이 B 주머니를 선택할 확률보다 2배 높을 것이다.
    - A주머니가 선택될 확률이 B 주머니보다 2배 높기 때문에 A주머니가 선택될 확률은 2/3, B 주머니가 선택될 확률은 1/3이다.
    - A주머니에서 빨간색 공이 나올 확률은 1, B주머니에서 빨간색 공이 나올 확률은 3/4이다.
    - A주머니에서 파란색 공이 나올 확률은 0, B주머니에서 파란색 공이 나올 확률은 1/4이다.
    - 따라서 빨간색 공이 나올 확률은 11/12, 파란색 공이 나올 확률은 1/12다.
    - 9개의 공을 꺼내서  빨간 공 8개와 파란 공 1개가 선택될 확률은 모델1을 기반으로 추측했을 때  (11/12)<sup>8</sup>*(1/12)=11<sup>8</sup>/12<sup>9</sup>
    - 모델2: A 주머니에는 빨간 공 4개가, B 주머니에는 빨간 공 3개와 파란 공 1개가 있다고 할 때 주머니를 선택 후 주머니 안에 있는 공을 선택하게 된다. 이 때 주머니 선택 단계에서 A 주머니를 선택할 확률이 B 주머니를 선택할 확률보다 2배 높다.
    - `모델2`: A주머니에는 빨간 공과 파란 공이 각기 2개씩, B주머니에는 빨간공이 3개, 파란공이 1개 들어있으며 이 때 주머니 선택 단계에서 A 주머니를 선택할 확률이 B 주머니를 선택할 확률보다 2배 높을 것이다.
    - 위와 동일한 방식으로 계산했을 때 빨간 공 8개와 파란 공 1개라는 데이터가 얻어질 확률은 5*7<sup>8</sup>/12<sup>9</sup>로 모델 1보다 낮은 확률이다.
    - 따라서 두 모델 중 `모델1`의 확률(likelihood)이 더 높으므로 `모델1`이 더 유망한(promising) 모델이라고 할 수 있다.



- EM Algorithm
  - likelihood를 최대화하는 파라미터를 찾기 위한 알고리즘
  - 데이터를 기반으로 cluster를 추측한다. 



## Probabilistic Latent Semantic Indexing(PLSI)

- EM Clustering보다 보다 복잡한 방식
  - A라는 사람이 글을 쓴다고 가정했을 때 이 사람이 어떤 글감을 선택하여 글을 쓸지를 확률로 나타낼 수 있다. 어떤 글감을 선택하느냐에 따라 글에 어떤 단어가 들어갈지에 대한 확률도 변화한다.
  - 여기서 A가 글에서 사용한 단어들은 관찰 가능한 데이터들이지만 어떤 주제를 선택할지, 어떤 단어를 선택할지는 관찰 불가능한 확률이다. 
  - 글에서 어떤 단어가 나올 때 마다 해당 단어가 나올 확률은 올라가게 되는 것이 되고 해당 단어가 많이 나오는 주제를 유추할 수 있게 된다. 사용된 단어를 통해 주제를 유추하는 것처럼 데이터를 통해 cluseter를 유추하는 방식이다.
- 예시
  - d1, d2 라는 2가지 문서가 있고 t1,t2라는 2가지 주제가 있으며 apple,banana라는 2가지 단어가 있을 다고 가정
  - d1에는 apple이라는 단어가, d2에는 banana라는 단어가 한 번은 들어있다는 사실을 알고 있음
  - 확률을 아래와 같이 무선적으로 할당(확률이므로 합은 반드시 1이어야 한다)
    - p(apple|t1)=0.6, p(banana|t2)=0.4
    - p(apple|t1)=0.3, p(banana|t2)=0.7
    - p(t1|d1)=0.6, p(t2|d1) =0.4
    - p(t1|d2)=0.3, p(t2|d2)=0.7
  - 이후 무선적으로 할당한 확률을 기반으로 계산(계산식은 추후 추가)







# Recommendation Systems

- Content based filtering method

  - item이나 product 등과 같은 actual content를 이용한다.
  - 각 item 간의 similarity를 이용하여 추천한다.

- Collaborative filtering method

  - 각각의 유저는 비슷한 다른 유저와 동일하게 행동한다는 가정, 다른 유저들이 추천에 영향을 미친다.

    - 다른 유저들의 의견을 이용, usage또는 preference pattern을 이용한다.

  - 유저가 직접 점수를 매긴 item들에 대한 rating을 이용해서 추천

  - Memory based method

    - 모델을 만드는 것이 아니라 여러 유저가 입력해온 데이터를 활용하는 방식

    - user-user similarity: 유저사이의 유사성을 파악, emory based method의 한 종류

      |      | p1   | p2   | p3   | p4   |
      | ---- | ---- | ---- | ---- | ---- |
      | c1   | 0    | 1    | 0    | 1    |
      | c2   | 0    | 1    | 1    | 1    |
      | c3   | 1    | 0    | 1    | 0    |

      |      | c1   | c2   | c3   |
      | ---- | ---- | ---- | ---- |
      | c1   | 1.00 | 0.82 | 0.00 |
      | c2   | 0.82 | 1.00 | 0.20 |
      | c3   | 0.00 | 0.20 | 1.00 |

      - c1-c2는 유사도가 높고, c1-c3는 유사도가 낮다.

      - c1과 가장 유사도가 높은 c2가 구매한 물건 중 아직 c1이 구매하지 않은 물건을 c1에게 추천해주게 된다. 아래 표는 위 두 표를 기반으로 생성한 표로 c1은 이미 구매한 p2,p4를 제외하고 가장 값이 큰 p3를 추천 받게 될 것이다.

        |      | p1   | p2   | p3   | p4   |
        | ---- | ---- | ---- | ---- | ---- |
        | c1   | 0.00 | 1.82 | 0.82 | 1.82 |
        | c2   | 0.20 | 1.82 | 1.20 | 1.82 |
        | c3   | 1.00 | 0.20 | 1.20 | 0.20 |

      

  - Model based methods

    - 유저 데이터베이스를 사용하여 모델을 생성하는 방식 

    - Matrix Factorization: Model based methods의 한 방식, 주로 영화 추천에 사용, 아래 표에서 행은 유저를 나타내고, 열은 영화를 나타내며 숫자는 영화에 매긴 평점을 나타낸다. 비어있는 칸은 유저가 아직 평점을 매기지 않은 영화이다.

      |      | m1   | m2   | m3   | m4   |
      | ---- | ---- | ---- | ---- | ---- |
      | u1   | 5    |      |      | 3    |
      | u2   | 4    |      | 5    | 2    |
      | u3   |      |      | 3    | 1    |
      | u4   | 4    | 4    |      |      |

      - 행의 유저 정보와 열의 영화 정보를 분리한후 유저가 기존에 입력한 평점을 기반으로 현 유저와 가장 유사한 유저와 현재 평점을 매긴 영화와 가장 유사한 영화를 각기 찾은 뒤 다시 합친다.
      - 이 결과를 통해 아직 평가하지 않은 영화에 예상 평점을 보여준다.







