# Tree

- Tree
  - 계층형 트리 구조를 시뮬레이션하는 추상 자료형(ADT)으로, 루트 값과 부모-자식 관계의 서브트리로 구성되며, 서로 연결된 노드의 집합이다.
  - 하나의 뿌리에서 뻗어나가는 형상처럼 생겨 트리라는 명칭이 붙었다.
    - 트리 구조를 표현할 때는 일반적인 나무의 형상과는 반대 방향으로 표현한다.
    - 즉, 최상단에 루트가 있다.
  - 트리의 특징
    - 재귀로 정의된(Recursively Defined) 자기 참조(Self-Referential) 자료구조이다. 즉, 트리는 자식도 트리고 그 자식토 트리이며, 여러 개의 트리가 모여 큰 트리가 된다.
    - 트리는 항상 단방향이기 때문에 간선의 화살표는 생략 가능하다.
    - 일반적으로 방향은 위에서 아래로 향하며 루트를 level 0로 둔다.
    - V개의 정점을 가진 트리는 V-1개의 간선을 가진다.
    - 임의의 두 정점을 연결하는 simple path(정점이 중복해서 나오지 않는 경로)가 유일하다.
  - 명칭
    - Root: 트리의 시작을 의미한다.
    - Leaf: 자식이 없는 정점을 의미한다.
    - Parent: 직접 연결된 두 노드에서 상위에 위치한 노드
    - Child: 직접 연결된 두 노드에서 하위에 위치한 노드
    - Degree: 자식 노드의 개수를 의미한다.
    - Size: 자신을 포함한 모든 자식 노드의 개수를 의미한다.
    - Height: 현재 위치에서부터 Leaf까지의 거리
    - Depth: 루트에서부터 현재 노드까지의 거리
  - 그래프와 트리
    - 트리는 그래프의 일종이다.
    - 트리는 그래프와 달리 순환 구조를 갖지 않는다.
    - 트리는 그래프와 달리 항상 단방향이다.
    - 트리에서 자식 노드는 항상 하나의 부모 노드만을 갖는다.
    - 트리에서 루트는 반드시 하나만 존재한다.



- 트리의 종류
  - m-ary 트리(다항 트리, 다진 트리)
    - 각 노드가 m개 이하의 자식을 갖고 있는 트리
  - 이진 트리
    - m-ary 트리에서 m=2일 경우, 즉 모든 노드의 차수가 2 이하인 트리.
    - 이진 트리는 왼쪽과 오른쪽 최대 2개의 자식을 갖는 매우 단순한 형태이다.
    - 대부분 트리라고 하면 이진 트리를 일컫는다.
  - 이진 트리 유형(Types of Binary Trees)
    - Full Binary Tree(정 이진 트리): 모든 노드가 0개 또는 2개의 자식 노드를 갖는다.
    - Complete Binary Tree(완전 이진 트리): 마지막 레벨을 제외하고 모든 레벨이 완전히 채워져 있으며, 마지막 레벨의 모든 노드는 가장 왼쪽에서부터 채워져 있는 트리
    - Perfect Binary Tree: 모든 노드가 2개의 자식 노드를 갖고 있으며, 모든 리프 노드가 동일한 깊이 또는 레벨을 갖는 트리.



- 트리 순회
  - 트리 순회란 그래프 순회의 한 형태로 트리 자료구조에서 각 노드를 정확히 한 번 방문하는 과정을 말한다.
  - BFS
    - 인접한 정점 중 부모만 제외하고 전부 queue에 넣은 후 방문하는 방식을 사용한다.
    - 부모를 제외하는 이유는 부모는 이미 탐색을 완료 했을 것이기 때문이다.
    - BFS를 사용하여 트리를 순회하면 각 정점을 높이 순으로 방문하게 된다.
  - DFS
    - DFS를 사용하여 트리를 순회하면 각 정점을 leaf에 도달할 때 까지 방문한 뒤 다시 돌아와 다른 노드를 방문한다.
    - 전위 순회, 중위 순회, 후위 순회가 있다.
  



- 이진 트리 순회

  ![image-20230418192309588](algorithm_part5.assets/image-20230418192309588.png)

  - 이진 트리의 표현
    - 이진 트리의 경우 왼쪽 자식과 오른쪽 자식을 구별해야 할 때가 종종 있어 아래와 같은 방식으로 표현하면 편리하다.
    - `lc`이 왼쪽 자식의 번호를 저장할 배열, `rc`는 오른쪽 자식의 번호를 저장할 배열이다.
    - 인덱스가 정점의 번호를 나타내며, 값이 0일 경우 해당하는 자식이 없다는 뜻이다.

  ```python
  lc = [0, 2, 4, 6, 0, 0, 0, 0, 0]
  rc = [0, 3, 5, 7, 0, 8, 0, 0, 0]
  ```

  - 전위 순회(Preorder Traversal, NLR(Node Left Righg))
    - 현재 정점 방문 - 왼쪽 서브 트리 전위 순회, 오른쪽 서브 트리 전위 순회 순서로 탐색한다.
    - DFS와 순회 순서가 동일하여, DFS를 통해 구현할 수 있다.

  ```python
  def preorder_traversal(root):
      if root == 0:
          return
      visited.append(root)
      preorder_traversal(lc[root])
      preorder_traversal(rc[root])
  
  lc = [0, 2, 4, 6, 0, 0, 0, 0, 0]
  rc = [0, 3, 5, 7, 0, 8, 0, 0, 0]
  visited = []
  preorder_traversal(1)
  print(visited)			# [1, 2, 4, 5, 8, 3, 6, 7]
  ```

  - 전위 순회 예시
    - 위 그래프를 예로 들자면, 먼저 1번 정점을 방문한다.
    - 그 후 왼쪽 전위 순회를 해야 하므로 2번 정점을 루트로 하는 서브 트리를 방문한다.
    - 또 다시 왼쪽 전위 순회를 해야 하므로 4번 정점을 방문한다.
    - 다시 왼쪽 전위 순회를 해야 하는데, 4번 정점에는 왼쪽 서브 트리가 없다.
    - 다음 순서인 오른쪽 전위 순회를 해야 하는데, 4번 정점에는 오른쪽 서브 트리도 없다.
    - 2로 돌아가 오른쪽 전위 순회를 진행하여 5번 정점을 방문한다.
    - 5번 정점에는 왼쪽 서브 트리가 없으므로 오른쪽 서브 트리로 진행한다.
    - 8은 왼쪽, 오른쪽 서브트리가 없으므로 이제 1로 돌아간다.
    - 1의 오른쪽 서브 트리의 루트인 3을 방문한다.
    - 3의 왼쪽 서브 트리의 루트인 6을 방문한다.
    - 6은 리프이므로 서브 트리가 없다.
    - 3으로 돌아가고 오른쪽 서브 트리의 루트인 7을 방문한다.
    - 7은 리프이므로 서브 트리가 없다.
    - 모든 정점을 순회했으므로 순회를 종료한다.
    - 방문 순서는 `[1, 2, 4, 5, 8, 3, 6, 7]`이 된다.

  - 중위 순회(Inorder Traversal, LNR)
    - 왼쪽 서브 트리 중위 순회 - 현재 정점 방문 - 오른쪽 서브트리 중위 순회순서로 탐색한다.
    - 중위 순회 역시 재귀를 통해 구현 가능하다.

  ```python
  def inorder_traversal(root):
      if root == 0:
          return
      inorder_traversal(lc[root])
      visited.append(root)
      inorder_traversal(rc[root])
  
  lc = [0, 2, 4, 6, 0, 0, 0, 0, 0]
  rc = [0, 3, 5, 7, 0, 8, 0, 0, 0]
  visited = []
  inorder_traversal(1)
  print(visited)			# [4, 2, 5, 8, 1, 6, 3, 7]
  ```

  - 후위 순회(Postorder Traversal, LRN)
    - 왼쪽 서브 트리 후위 순회 - 오른쪽 서브 트리 후위 순회 - 현재 정점 방문 순서로 탐색한다.
    - 마찬가지로 재귀로 구현이 가능하다.

  ```python
  def postorder_traversal(root):
      if root == 0:
          return
      postorder_traversal(lc[root])
      postorder_traversal(rc[root])
      visited.append(root)
      
  lc = [0, 2, 4, 6, 0, 0, 0, 0, 0]
  rc = [0, 3, 5, 7, 0, 8, 0, 0, 0]
  visited = []
  postorder_traversal(1)
  print(visited)          # [4, 8, 5, 2, 6, 7, 3, 1]
  ```
  
  - 모든 순회에서 오른쪽 서브 트리보다 왼쪽 서브트리를 먼저 방문하는 이유
    - 만약 단순히 노드의 개수를 세기 위함이라면 오른쪽 서브트리를 먼저 방문해도 상관 없다.
    - 그러나 아래에서 살펴볼 BST는 왼쪽 서브트리의 값으 항상 오른쪽 서브트리보다 작게 정렬되어 있다.
    - 따라서 오름차순으로 노드들을 방문하기 위해서는 왼쪽 서브트리 먼저 방문해야 한다.






## Binary Search Tree(BST)

- 이진 탐색 트리

  - 왼쪽 서브 트리의 모든 값은 부모의 값 보다 작고, 오른쪽 서브 트리의 모든 값은 부모의 값 보다 큰 이진 트리를 의미한다.
  - 예시
    - 아래 트리는 모든 노드의 차수가 2 이하이므로 이진 트리이다.
    - 아래 트리에서 왼쪽 서브 트리의 모든 값은 부모의 값 보다 작고, 오른쪽 서브 트리의 모든 값은 부모의 값 보다 크다.
    - 따라서 아래 트리는 이진 탐색 트리이다.

  ![image-20230505113617367](algorithm_part5.assets/image-20230505113617367.png)



- 이진 탐색 트리의 시간복잡도

  - 삽입, 조회, 수정, 삭제가 모두 O(lg N)에 가능하다(N은 tree의 높이이다).

    - 배열의 경우 삽입은 맨 마지막에 추가하면 되니 O(1)이지만, 조회, 수정, 삭제는 모두 O(N)이다.
    - 모든 연산이 O(1)에 가능한 hash의 하위호환이라 생각할 수도 있지만 그렇지 않다.
    - 이진 탐색 트리는 hash와 달리 원소가 크기 순으로 정렬된다는 특징을 가지고 있어 N보다 크거나 작인 최초의 원소를 O(lg N)에 찾을 수 있다.
    - 즉, 원소의 조회, 수정, 삭제가 빈번하면서 원소의 대소와 관련된 연산을 해야 할 경우 이진 탐색 트리를 사용하는 것이 좋다.

  - 삽입

    - 이진 탐색 트리에서의 삽입 과정

    - 값이 50인 root node가 있을 때, 15를 삽입한다면, 15는 50보다 작으므로 root node의 왼쪽에 삽입한다.
    - 다음으로 20을 삽입하려 하는데, 20 역시 50보다 작으므로 왼쪽에 삽입하려 한다.
    - 그런데 이미 왼 쪽에는 15가 있으므로 바로 삽입할 수는 없고 20과 15의 대소를 비교한다.
    - 20은 15보다는 크므로 20의 오른쪽에 삽입한다.
    - 이러한 과정을 거쳐 삽입이 이루어진다.

  - 조회

    - 위 예시에서 값이 4인 노드를 찾으려고 한다고 가정해보자.
    - 4는 root node인 5보다 작으므로 왼쪽 서브트리에 있을 것이다.
    - 4는 왼쪽 서브 트리의 root인 3보다는 크므로 오른쪽에 있을 것이다.
    - 찾으려는 노드를 찾았으므로 탐색을 종료한다.

  - 삭제

    - 그냥 삭제할 경우 트리 구조가 깨지므로 주의해야한다.
    - 자식이 없는 정점의 경우 그냥 지워도 문제가 되지 않는다.
    - 자식이 1개인 정점을 지우는 것도 상대적으로 간단한데, 지우려는 정점을 지운 후 자식 정점을 지워진 정점의 위치에 놓으면 끝이다.
    - 반면에 자식이 2개인 정점을 지우기 위해서는 적절한 정점을 선택하여 지워진 정점의 위치에 놓는 것이다.
    - 이 때, 지워진 정점 보다 크면서  가장 작은 값을 가지는 정점을 선택하거나, 지워진 정점 보다 작으면서 가장 큰 값을 가지는 정점을 선택하면 된다.
    - 예를 들어 위 예시에서 3번 정점을 삭제하려 한다면, 3보다 작으면서 가장 큰 2를 3번 정점 위치로 올리거나, 3보자 크면서 가장 작은 4를 3번 정점 위치로 올리면 이진 탐색 트리의 구조가 유지된다.
    - 그런데, 만약 2 또는 4가 2개의 자식을 가지고 있다면 어떻게 되는가?
    - 사실 그런 상황 자체가 만들어질 수 없다.
    - 만약 2에게 1 이외에 오른쪽 자식이 있었다면, 2보다 크면서 3보다는 작은 값일 텐데, 그렇다면 대체 정점으로 2가 선택되지 못한다.
    - 마찬가지로 4에게 자식 둘이 있었다면 왼쪽 자식은 3보다 크고 4보다 작은 값이 들어가게 될 텐데, 이 경우 역시 대체 정점으로 4가 선택되지 못한다.



- 자가 균형 트리(Self-Balancing Tree)

  - 이진 탐색 트리의 시간 복잡도는 각 정점의 자식의 수에 따라 정해진다.
    - 만약 각 정점이 대부분 2개씩 가지고 있다면 높이가 하나 증가할 때 마다 정점의 개수는 2배씩 늘어나기 때문에 시간 복잡도가 O(lg N)에 가까워진다.
    - 반면에 각 정점이 대부분 1개씩 가지고 있다면 시간복잡도는 O(N)에 가까워진다.
    - 즉, 아래와 같이 같은 개수의 정점이라도, 자식을 몇 개씩 가지고 있는가에 따라 높이가 달라지게 된다.
    - 아래 두 이진 탐색 트리는 같은 정점의 개수를 가지고 있지만, 위는 높이가 3, 아래는 높이가 6이다.
    - 따라서 트리가 편향되었다면, 연결 리스트를 사용하는 것과 별반 차이가 없어지게 된다.

  ![image-20230505133825377](algorithm_part5.assets/image-20230505133825377.png)

  ![image-20230505133841239](algorithm_part5.assets/image-20230505133841239.png)

  - 그러므로 이진 검색 트리가 편향 될 경우 이를 해결해줘야하는데, 이를 해결한 트리를 자가 균형 트리라 부른다.
    - AVL Tree, Red Black Tree 등이 자가 균형 트리이다.
    - 구현은 AVL Tree가 약간 더 쉽고, 성능은 Red Black Tree가 더 좋아서 Red Black Tree를 주로 사용한다.





# Heap

- 힙의 특성을 만족하는 거의 완전한 트리(Almost Complete Tree)인 특수한 트리 기반의 자료 구조

  - J.W.J Wiliams가 힙 정렬 알고리즘을 고안하면서 설계했다.
  - 이진 힙(Binary heap)
    - 본래 완전 이진 트리인 힙을 이진 힙이라고 부르지만 일반적으로 힙이라 하면 이진 힙을 의미한다.
  - 최대 힙
    - 부모 노드의 값이 항상 자식 노드보다 큰 힙을 최대 힙이라 부른다.
    - 최댓값을 찾기 위해 사용한다.
  - 최소 힙
    - 부모 노드의 값이 항상 자식 노드보다 작은 힙을 최소 힙이라 부른다.
    - 최솟값을 찾기 위해 사용한다.

  - 주의할 점은 힙의 특성(부모가 자식보다 크다/작다)에 정렬되어 있어야 한다는 것은 없다는 점이다.
    - 즉 부모가 자식보다 크다/작다는 조건만 만족할 뿐 정렬된 형태로 트리가 구성되지는 않는다.
  - 높이가 낮은 곳 부터 채워 나가며, 높이가 같을 경우 왼쪽부터 채워 나간다.
    - 따라서 이진 탐색 트리와 달리 불균형이 발생하지 않는다.



- 최소 힙의 삽입과 삭제

  - 삽입

    - 먼저 root 값인 42를 삽입한다.
    - 다음으로 50을 삽입하는데, 힙은 무조건 높은 곳에서 낮은 곳으로, 왼쪽에서 오른쪽으로 채워 나가므로 root의 왼쪽에 삽입한다.
    - 다음으로 30을 삽입하는데, 왼쪽에서 오른쪽으로 채워야 하므로 root의 오른쪽에 삽입해야 한다.
    - 그런데, 30은 root인 42보다는 작으므로 30을 42의 오른쪽에 둘 경우 최소힙의 조건에 위배된다.
    - 따라서 42와 30의 자리를 바꿔준다.

    ![image-20230510212422155](algorithm_part5.assets/image-20230510212422155.png)

    - 다음으로는 12를 삽입하려 하는데, 왼쪽에서 오른쪽으로 채워야 하므로 50의 왼쪽 노드로 와야 한다.
    - 그런데 이 역시 최소 힙의 조건을 위배하므로, 50과 자리를 바꿔준다.

    ![image-20230510212610038](algorithm_part5.assets/image-20230510212610038.png)

    - 12는 30보다 작으므로 여전히 최소 힙의 조건을 위배하므로 12와 30도 자리를 바꿔준다.

    ![image-20230510212656527](algorithm_part5.assets/image-20230510212656527.png)

  - 삭제

    - 아래와 같은 heap이 있을 때, 최솟값인 12를 지우려고 한다.

    ![image-20230510214303059](algorithm_part5.assets/image-20230510214303059.png)

    - Heap의 가장 마지막 노드인 30과 12의 자리를 바꾸고 12를 제거한다.

    ![image-20230510214326553](algorithm_part5.assets/image-20230510214326553.png)

    - 자식이 부모보다 커야 한다는 최소 힙의 조건에 위배되므로 자식 노드들 중 가장 작은 것과 다시 변경한다.
    - 13과 14중 13이 더 작으므로 13과 변경한다.
    - 아직도 자식인 16, 17보다 크므로 둘 중 더 작은 자식인 16과 변경한다.

    ![image-20230510214457383](algorithm_part5.assets/image-20230510214457383.png)

    - 이제 두 자식 41, 43보다는 크므로 최소 힙의 조건을 만족하므로 변경을 종료한다.

  - 시간복잡도

    - 삽입의 경우 값을 계속 비교한다고 해도 최대 높이까지만 올라가면서 노드를 변경하면 되므로 O(lg N)이다.
    - 삭제의 경우도 마찬가지 이유로 O(lg N)이다.
    - 최소 힙에서 최솟값을 찾는 연산은 root를 확인하면 되므로 O(1)이다(최대 힙에서 최댓값을 찾는 연산도 마찬가지).



- 표현

  - 힙은 완전 이진 트리이므로 list에 빈틈 없이 배치가 가능하다.
    - 대개의 경우 계산을 편하게 하기 위해 1번 인덱스부터 사용한다.
  - 위에서 삭제 방식을 확인하기 위해 사용했던 heap을 기준으로 아래와 같은 배열로 표현 가능하다.

  ```python
  heap = [None, 13, 16, 14, 30, 17, 22, 24, 41, 43, 31, 25, 26, 26, 27]
  ```

  - 이 경우 i번 인덱스에 해당하는 노드의 부모와 왼쪽 자식, 오른쪽 자식은 아래와 같이 찾을 수 있다.
    - 부모: `i / 2`
    - 왼쪽 자식: `2i`
    - 오른쪽 자식:`2i + 1`



- Python의 heapq

  - Python에는 `heapq`라는 모듈로 최소 힙이 구현되어 있다.
  - Python의 list를 최소 힙 처럼 다룰 수 있게 해준다.

  ```python
  import heapq
  
  heap = []
  heap.heappush(3)
  heap.heapush(1)
  print(heap.heappop())	# 1
  ```



- 이진 탐색 트리와의 차이
  - 이진 힙은 부모 자식 사이의 대소 관계를 보장하지만, 이진 탐색 트리는 좌우 사이의 대소 관계를 보장한다.
  - BST는 탐색과 삽입 모두 O(lg N)에 가능하며, 모든 값이 정렬되어야 할 때 사용한다.
  - 이진 힙은 가장 작은 값/ 가장 큰 값을 빈번히 추출해야 할 때 사용한다.
    - 우선 순위와 연결되어 있으며 우선 순위 큐에 사용된다.







# 에라스토테네스의 체

- 에라스토테네스의 체

  - 소수(prime number)를 판별하는데 사용하는 알고리즘이다.
    - 고대 그리스의 수학자 에라스토테네스가 발견하였다.
    - 체 처럼 소수가 아닌 수 들을 걸러낸다.
  - 알고리즘
    - 2부터 소수를 구하고자 하는 구간의 모든 수를 나열한다.
    - 나열한 수를 앞에서부터 탐색해 나간다.
    - 첫 수인 2는 소수이므로 소수라는 표시를 한다.
    - 나열 된 숫자들에서 2의 배수인 것들은 모두 소수가 아니라는 표시를 한다.
    - 3은 표시가 되어 있지 않고, 소수이므로 소수라는 표시를 한다.
    - 나열 된 숫자들에서 3의 배수인 것들은 모두 소수가 아니라는 표시를 한다.
    - 4는 소수가 아니라는 표시가 되어 있으므로 넘어간다.
    - 5는 표시가 되어 있지 않고, 소수이므로 소수라는 표시를 한다.
    - 나열 된 숫자들에서 5의 배수인 것들은 모두 소수가 아니라는 표시를 한다.
    - 이를 모든 수에 대해 반복한다.
  - 구현

  ```python
  def prime_list(n):
      # 에라토스테네스의 체 초기화: n개 요소에 True 설정(소수로 간주)
      sieve = [True] * n
  
      # n의 최대 약수가 sqrt(n) 이하이므로 i=sqrt(n)까지 검사
      m = int(n ** 0.5)
      for i in range(2, m + 1):
          if sieve[i] == True:           # i가 소수인 경우
              for j in range(i+i, n, i): # i이후 i의 배수들을 False 판정
                  sieve[j] = False
  
      # 소수 목록 산출
      return [i for i in range(2, n) if sieve[i] == True]
  ```

  - 주의사항

    > [참고](https://nahwasa.com/entry/%EC%97%90%EB%9D%BC%ED%86%A0%EC%8A%A4%ED%85%8C%EB%84%A4%EC%8A%A4%EC%9D%98-%EC%B2%B4-%ED%98%B9%EC%9D%80-%EC%86%8C%EC%88%98%ED%8C%90%EC%A0%95-%EC%8B%9C-%EC%A0%9C%EA%B3%B1%EA%B7%BC-%EA%B9%8C%EC%A7%80%EB%A7%8C-%ED%99%95%EC%9D%B8%ED%95%98%EB%A9%B4-%EB%90%98%EB%8A%94-%EC%9D%B4%EC%9C%A0)

    - n까지의 소수 판별시에 n의 제곱근까지만 확인하면 된다(위 코드에서도 `n ** 0.5`까지만 확인했다).
    - n은 자연수 a, b에 대해 `n = a * b`라고 표현할 수 있다.
    - 또 n의 제곱근 m에 대해 `n = m * m`라고 표현할 수 있다.
    - 따라서, `a * b = m * m`이라 할 수 있다.
    - 이 때, a, b는 자연수여야하므로, a, b가 자연수임을 만족하는 경우는 아래의 세 가지 경우 뿐이다.
    - `a=m & b=m`, `a<m & b>m`, `a>m & b<m`
    - 즉, `min(a, b)<=m`이라고 할 수 있다.
    - N의 약수에 해당하는 a와 b 중 하나는 무조건 m 이하이므로, m까지만 조사하면 n이 소수인지 알 수 있게 된다.





# 연결리스트

- Array와 list

  > 자료구조로서의 array, list와 프로그래밍 언어에서의 array, list를 분리해서 생각해야한다.
  >
  > 예를 들어 C의 영향으로 array는 길이를 변경할 수 없다고 생각하지만, 자료구조로서의 array는 단순히 메모리상에 원소를 연속하여 배치한 구조로, 길이를 변경하지 못할 이유가 없다.

  - array
    - 일반적으로 배열이라 번역되는 array는 메모리상에 원소를 연속하게 배치한 자료구조이다.
    - 메모리상에 연속적으로 배치하므로, index를 사용하여 k번째 원소를 상수 시간에 찾는 것이 가능하다.
    - 추가적으로 소모되는 메모리의 양(overhead)가 거의 없다.
    - 메모리상에 연속된 구간에 할당해야하므로 할당에 제약이 존재한다(C는 이러한 제약을 극복하기 위해 array의 길이를 변경하지 못하게 설계됐다).
    - Cache hit rate가 높다.
  - array의 시간복잡도
    - index를 알고 있을 경우 해당 index에 해당하는 값에 접근하는 것과 변경하는 것의 시간복잡도는 O(1)이다.
    - 임의의 위치에 자료를 추가하거나 삭제하는 연산의 시간복잡도는 O(n)이다. 추가, 삭제 이후 element들을 한 칸씩 당기거나 밀어야 하기 때문이다.
    - array의 마지막에 원소를 추가, 삭제하는 경우의 시간복잡도는 O(1)이다.

  - list
    - 메모리상에 원소를 불연속적으로 배치한 자료구조이다.
    - 메모리상에 원소를 불연속적으로 배치하므로 index를 통한 접근이 불가능하다.
    - 배열과 유사한 역할을 하지만 차이가 있다면 빈 element를 허용하지 않는다는 것이다.
    - 빈틈없는 데이터의 적재가 가능해 낭비되는 메모리가 거의 없다.
    - 배열과 마찬가지로 원소들 사이의 순서가 존재하며, 또한 배열과 마찬가지로 중복을 허용한다.
    - 연속되어 배치되지 않으므로 Cache hit rate가 낮다.



- 연결리스트
  - 데이터 요소의 선형 집합으로, 데이터의 순서가 물리적인 순서대로 저장되지 않는다.
    - 컴퓨터과학에서 배열과 함께 가장 기본이 되는 대표적인 선형 자료구조 중 하나로 다양한 추상 자료형(Abstract Data Type, ADT) 구현의 기반이 된다.
    - 동적으로 새로운 노드를 삽입하거나 삭제하기가 간편하며, 연결 구조를 통해 물리 메모리를 연속적으로 사용하지 않아도 되기 때문에 관리도 쉽다.
  - 랜드 연구소에서 근무하던 앨런 뉴얼이 동료들과 함께 만든 언어인 IPL의 기본 자료구조로 처음 사용됐다.
  - 연결 리스트의 성질
    - 배열과는 달리 특정 인덱스에 접근하기 위해서는 전체를 순서대로 읽어야하므로 상수 시간에 접근할 수 없다.
    - 메모리상에 연속되어 배치되지 않으므로 cache hit rate가 낮다.
    - 각 원소가 다음 원소, 혹은 이전과 다음 원소의 주소값을 가지고 있어야하므로, 추가적인 메모리 공간(overhead)이 요구된다. 
    - 예를들어 32비트 컴퓨터면 주소값이 32비트(=4바이트) 단위이니 4N 바이트가 추가로 필요하고, 64비트 컴퓨터라면 주소값이 64비트(=8바이트) 단위이니 8N 바이트가 추가로 필요하게 된다. 즉 N에 비례하는 만큼의 메모리를 추가로 쓰게 된다.'
  - 시간 복잡도
    - 탐색과 변경에는 O(n)이 소요된다.
    - 반면, 시작 또는 끝 지점에 아이템을 추가, 삭제, 추출하는 작업은 상수 시간에 가능하다.
    - 시작 또는 끝 지점이 아닌 임의의 공간에 추가와 삭제를 하는 경우 추가 또는 삭제 할 곳의 주소를 알고 있을 때에만 O(1)이다.
    - 예를 들어 1->34->17->22와 같은 연결 리스트가 있을 때, 세 번째 원소 뒤에 61을 추가하려 한다면 이는 상수 시간 내에는 불가능하다.
    - 세 번째 원소가 어디인지를 찾는데 시간이 소요되기 때문이다.
    - 단, 주소를 알고 있을 경우에는 단순히 추가하려는 원소 앞의 원소가 가리키는 주소를 추가한 원소의 주소로 바꾸고, 추가한 원소가 가리키는 주소를 뒤의 원소로 바꿔주기만 하면 된다.



- 연결 리스트의 종류
  - 단일 연결 리스트(Singly linked list)
    - 각 원소가 다음 원소의 주소를 가지고 있는 연결 리스트
  - 이중 연결 리스트(Doubly linked list)
    - 각 원소가 자신의 다음 원소의 주소와 이전 원소의 주소를 가지고 있는 연결 리스트
    - 이전 원소의 정보를 알 수 있다는 장점이 있지만, 단일 연결 리스트에 비해 추가적인 메모리가 필요하다는 단점이 있다.
  - 원형 연결 리스트(Circular linked list)
    - 처음 원소와 마지막 원소가 연결되어 있는 연결 리스트
    - 단일 연결 리스트이면서 원형 연결 리스트일 수도, 이중 연결 리스트이면서 원형 연결 리스트일 수도 있다.



- Floyd's cycle finding algorithm(Hare-Tortoise algorithm)

  - 단일 연결 리스트에 cycle이 존재하는지를 판별하는 알고리즘이다.

    - 거치는 모든 노드를 저장 할 필요 없이, 포인터 두 개만 있으면 되므로 공간복잡도 O(1)에 해결이 가능하다.
    - 시간 복잡도는 O(n)이다.

  - 방식

    - 한 칸씩 전진하는 포인터(slow pointer, tortoise)와 두 칸씩 전진하는 포인터(fast pointer, hare)를 동일한 시작점에서 출발시킨다.
    - 만일 연결 리스트에 순환이 존재할 경우 두 포인터는 반드시 만나게 된다.
    - 만약 순환이 존재하지 않을 경우 fast pointer가 연결리스트의 끝에 도달하게 된다.

  - 이를 사용하여 cycle의 시작점을 찾을 수도 있다.

    - 한 칸씩 전진하는 포인터(slow pointer, tortoise)와 두 칸씩 전진하는 포인터(fast pointer, hare)를 동일한 시작점에서 출발시킨다.
    - 두 포인터가 만나게 되면 둘 중 한 포인터를 시작점으로 돌려보낸다.
    - 이제 두 포인터 모두 한 칸씩 전진시키면, 두 포인터가 만나는 지점이 순환의 시작점이다.

  - cycle의 시작점을 찾는 원리

    - y는 두 포인터의 시작점부터 순환의 시작점까지의 거리이다.
    - z는 순환의 시작점부터 두 포인터가 만나는 지점까지의 거리이다.
    - l는 순환의 길이이다.
    - f는 fast pointer가 순환을 돈 횟수, s는 slow pointer가 순환을 돈 횟수이다.
    - $x_n$은 연결 리스트의 n번째 노드이다.
    - $x_i$는 두 포인터가 만나는 노드이다.
    - $x_j$는 순환 내부에 있는 노드이며, 순환 내부에 있는 노드 $x_j$에 대해 아래의 공식이 성립한다. 즉, node가 순환 내에 존재한다면 몇 순환을 몇 번 돌아도 같은 지점에 도착하게 된다.

    $$
    x_{j+kl} = x_j\ \ (j \geλ\ and \ k\ge0)
    $$

    

    - 이 때 두 포인터가 만날 때 까지 slow pointer가 이동한 거리 i는 아래와 같다.

    $$
    i = y+(s*l)+z
    $$

    - fast pointer는 slow pointer의 두 배씩 이동하므로, 두 포인터가 만날 때 까지 fast pointer가 이동한 거리 2i는 아래와 같다.

    $$
    2i = y+(f*l)+z
    $$

    - 2i에서 i를 빼면 아래와 같다.

    $$
    i = (f-s)l
    $$

    - 이제 첫 번째 식에서 j에 y를 대입한다.
    - y는 연결 리스트의 시작점부터 순환의 시작점까지의 거리이므로, $x_y$가 가리키는 node는 순환이 시작되는 node이고, 따라서 순환 내부에 있는 node라 할 수 있다.
    - 그 다음 k값에는 fast pointer가 순환을 돈 횟수에서 slow pointer가 순환을 돈 횟수를 뺀 f-s를 대입하면 식은 아래와 같다.

    $$
    x_{y+(f-s)l} = x_y
    $$

    - 여기서 우리는 `(f-s)l`의 값이 i라는 것을 알고 있으므로, 식은 아래와 같이 변경될 수 있다.

    $$
    x_{y+i}=x_y
    $$

    - 즉 두 포인터가 만나는 지점($x_i$)에서 y만큼 이동하면 사이클이 시작되는 지점($x_{y+i}$)로 돌아갈 수 있다.

  - Runner technique

    - 두 개의 pointer를 동일한 시작점에서 서로 다른 속도로 출발시킨다는 아이디어로 다양한 문제 해결이 가능하다.
    - 예를 들어 cycle이 없는 연결 리스트에서 fast pointer가 끝에 도달하면, slow list는 중간 지점에 도달하게 되는데, 이를 사용하여 회문 판별등을 할 수 있다.





# Trie

- Trie

  - 검색 트리의 일종으로 일반적으로 키가 문자열인, 동적 배열 또는 연관 배열을 저장하는 데 사용되는 정렬된 트리 자료구조다.
    - 자연어 처리 분야에서 문자열 탐색을 위한 자료구조로 널리 쓰인다.
    - 검색시에 자동완성 기능을 구현하는 용도로도 많이 쓰인다.
    - 검색을 뜻하는 retrieval의 중간 음절에서 용어를 따 왔다(초기에는 트리로 발음했으나 tree와 구분하기 위해 트라이로 발음한다).
    - 문자열의 길이만큼만 탐색을 하면 찾는 문자열이 트라이 내에 있는지 확인 할 수 있다.
  - 예시

  ![image-20230511204402225](algorithm_part5.assets/image-20230511204402225.png)

  - 단어 S를 삽입, 조회, 삭제하는 시간 복잡도는 O(|S|)이다.
    - `|S|`는 S의 길이를 의미한다.
    - 이론적인 시간복잡도는 O(|S|)이지만, 실질적으로는 해시, 이진 탐색 트리에 비해 훨씬 느리다.
    - 따라서 일반적인 상황에서는 해시, 이진 탐색 트리를 사용하고 트라이가 꼭 필요한 상황에서만 트라이를 사용하는 것이 좋다.
  - 메모리를 많이 차지한다는 단점이 있다.
    - 문자열을 일반 배열에 저장하는 것 보다 `글자의 종류 * 4 `배 만큼을 더 사용한다.
    - 예를 들어 대문자 알파벳으로만 구성된 단어들을 저장할 경우 (26*4=)104배 만큼의 메모리를 더 사용한다.
    - 그렇다고 메모리 사용량을 줄이는 방식을 적용하면 연산 속도가 느려진다.



- 삽입 예시

  - 삽입, 탐색, 삭제 시에 노드를 옮겨 다녀야 하므로 현재 정점이 어디인지가 중요하다.
  - 아래와 같이 root 노드만 있을 때, APPLE을 삽입하려 한다.
    - 현재 정점은 root

  ![image-20230511210821075](algorithm_part5.assets/image-20230511210821075.png)

  - 첫 글자인 A를 먼저 삽입하려 하는데 root는 값이 A인 자식이 없으므로 A를 자식으로 추가한다.
    - 그 후 현재 정점을 A로 옮긴다.

  ![image-20230511211011254](algorithm_part5.assets/image-20230511211011254.png)

  - 다음으로 P를 추가하는데, 마찬가지로 A 정점에는 값이 P인 자식이 없으므로 P를 현재 정점인 A의 자식으로 추가한다.
    - 현재 정점을 P로 옮긴다.

  ![image-20230511211253932](algorithm_part5.assets/image-20230511211253932.png)

  - 위 과정을 반복하여 남은 P, L, E를 아래와 같이 추가해준다.
    - 마지막 문자인 E를 추가한 뒤 E 노드에 단어의 끝이라는 표시를 해야 한다.

  ![image-20230511211526373](algorithm_part5.assets/image-20230511211526373.png)

  - 다음으로는 APPLY를 추가하려 한다.
    - 다시 현재 정점을 root로 옮긴다.
    - 현재 정점인 root에는 첫 글자인 A를 값으로 가지는 자식이 있으므로, 추가는 하지 않고 현재 정점을 A로 옮긴다.
    - 현재 정점인 A에는 그 다음 글자인 P를 값으로 가지는 자식이 있으므로, 추가는 하지 않고 현재 정점을 P로 옮긴다.
    - 같은 과정을 반복한다.
  - 마지막 Y를 추가하려고 하는데 현재 정점 L에는 Y를 값으로 가지는 자식이 없으므로, L의 자식으로 Y를 추가해준다.
    - Y를 추가한 뒤 단어의 끝이라는 표시를 Y 노드에 남겨야 한다.

  ![image-20230511211811730](algorithm_part5.assets/image-20230511211811730.png)

  - 단 주의할 점은 삽입이 끝났다면 마지막 노드(위 예시에서 E, Y)에 단어의 끝이라는 표시를 해줘야한다.
    - 예를 들어 BANANA와 BAN을 삽입하려 한다고 가정해보자.
    - BANANA를 먼저 삽입하면 트라이는 B-A-N-A-N-A와 같인 형태가 될 것이다.
    - 그 이후 BAN도 삽입하는데, B-A-N이 이미 있는 상태이므로 더 추가해 줄 것은 없다.
    - 이 후 트라이에 BAN이 있는지 확인하려 한다.
    - 그런데 별도의 표시를 해주지 않았으므로 B-A-N이 BANANA의 일부로 들어간 것인지 BAN을 삽입한 것인지 알 방법이 없다.
    - 따라서 하나의 단어가 삽입이 완료 됐을 때 특정 표시를 해 줌으로써 단어가 삽입됐다는 것을 기록해둬야 한다.



- 조회 예시
  - 조회도 삽입과 크게 다르지 않다.
  - 찾으려는 값이 있을 경우
    - 위에서 본 예시에서 APPLE을 검색하려 한다고 가정해보자.
    - 먼저 A의 첫 글자인 A가 현재 정점인 root의 자식 중에 있는지 확인한다.
    - root의 자식 중 A가 있으므로 현재 정점을 A로 옮긴다.
    - 다음 글자인 P가 현재 정점인 A의 자식 중에 있는지 확인한다.
    - A의 자식 중 P가 있으므로 현재 정점을 P로 옮긴다.
    - 위 과정을 반복하여 E에 도달하고, E에 단어의 끝이라는 표식이 있으므로 APPLE은 트라이 안에 있다는 것을 확인할 수 있다.
  - 찾으려는 값이 없을 경우
    - API라는 문자열이 트라이 안에 있는지 확인하려 한다.
    - 먼저 A의 첫 글자인 A가 현재 정점인 root의 자식 중에 있는지 확인한다.
    - root의 자식 중 A가 있으므로 현재 정점을 A로 옮긴다.
    - 다음 글자인 P가 현재 정점인 A의 자식 중에 있는지 확인한다.
    - A의 자식 중 P가 있으므로 현재 정점을 P로 옮긴다.
    - 다음 글자인 I가 P의 자식중에 없으므로 API는 트라이에 없다는 것을 확인할 수 있다.
  - 찾으려는 값은 있지만 종료 표시는 없는 경우
    - 위 트리에서 APP을 찾으려 한다고 가정해보자.
    - 먼저 A의 첫 글자인 A가 현재 정점인 root의 자식 중에 있는지 확인한다.
    - root의 자식 중 A가 있으므로 현재 정점을 A로 옮긴다.
    - 다음 글자인 P가 현재 정점인 A의 자식 중에 있는지 확인한다.
    - A의 자식 중 P가 있으므로 현재 정점을 P로 옮긴다.
    - P의 자식 중 P는 있지만 자식인 P 정점에 단어의 끝이라는 표시가 되어 있지 않으므로 APP은 트라이에 없다는 것을 확인할 수 있다.



- 삭제
  - APPLE을 삭제하려 한다고 가정해보자.
    - 조회를 통해 APPLE을 탐색한다.
    - 최종 위치인 E가 현재 정점이 된다.
    - 현재 정점이자 단어의 마지막 문자에 해당하는 E 정점에서 단어의 끝이라는 표시를 지운다.
  - 주의할 점은 정점을 지우지는 않는다는 것이다.
    - 정점을 지울 경우 트라이의 구조 자체가 깨지기 때문에 트라이는 문자열이 삭제된다고 하더라도 해당 정점들을 삭제하지 않는다.
    - 이로 인해 메모리가 낭비된다.
  - 삭제가 빈번하게 발생할 경우 트라이를 사용하는 것은 적절치 않다.





# 정렬

- 버블 정렬(Bubble Sort)

  - 구현 가능한 정렬 알고리즘 중 가장 느린 알고리즘이다.
  - 매번 앞뒤의 원소를 비교해가며 정렬하는 알고리즘이다.
    - 한 번 반복문이 돌 때마다 가장 큰 원소가 배열의 가장 뒤로 가게 된다.
  - 시간복잡도
    - 매 번 모든 원소들을 비교해야 하기 때문에 O(n<sup>2</sup>)이다.

  - 구현

  ```python
  def bubble_sort(arr):
      length = len(arr)
      for i in range(1, length):
          for j in range(length-i):
              if arr[j] > arr[j + 1]:
                  arr[j], arr[j+1] = arr[j+1], arr[j]
  
  lst = [4, 2, 3, 5, 6, 1, 4, 9, 8, 7]
  bubble_sort(lst)
  ```



- 선택 정렬(Selection Sort)

  - 제자리 정렬(In-Place Sort) 알고리즘 중 하나이다.
    - 제자리 정렬 알고리즘이란 정렬할 원소들의 개수에 비해서 충분히 무시할 만한 저장 공간만을 필요로 하는 정렬 알고리즘을 말한다.
  - 정렬 방식
    - 원소들 중 가장 작은 값을 찾는다.
    - 해당 값을 배열의 가장 첫 원소와 바꾼다.
    - 맨 처음 위치를 뺀 배열을 같은 방식으로 교체한다.
  - 시간복잡도
    - 매 번 남은 배열 전체를 순회하면서 가장 작은 값을 찾아야 하기에 O(n<sup>2</sup>)이다.
    - 같은 O(n<sup>2</sup>)인 버블 정렬보다는 항상 우수하다.
  - 구현

  ```py
  def selection_sort(arr):
      length = len(arr)
      for i in range(length - 1):
          index_min = i
          for j in range(i + 1, length):
              if arr[index_min] > arr[j]:
                  index_min = j
          arr[i], arr[index_min] = arr[index_min], arr[i]
  
  
  lst = [4, 2, 3, 5, 6, 1, 4, 9, 8, 7]
  selection_sort(lst)
  ```



- 삽입 정렬(Insertion Sort)

  - 정렬 방식
    - 특정 인덱스의 앞에 있는 값들을 정렬된 값, 뒤에 있는 값들을 정렬되지 않은 값으로 본다.
    - 0번 인덱스는 정렬되었다고 가정하고 1번 인덱스부터 탐색을 시작한다.
    - 특정 인덱스를 이미 정렬된 배열(즉, 특정 인덱스 앞의 값들)의 적절한 위치에 삽입하는 방식으로 정렬한다.
  - 시간복잡도
    - 위의 두 정렬 알고리즘과 마찬가지로 O(n<sup>2</sup>)이다.
    - 선택 정렬에 비해 조금 더 효율적이다.
  - 구현

  ```python
  def insert_sort(arr):
      for i in range(1, len(arr)):
          for j in range(i, 0, -1):
              if arr[j - 1] > arr[j]:
                  arr[j - 1], arr[j] = arr[j], arr[j - 1]
  
  
  lst = [4, 2, 3, 5, 6, 1, 4, 9, 8, 7]
  insert_sort(lst)
  ```



- 병합 정렬(Merge Sort)

  - 분할 정복(Divide and Conquer) 알고리즘을 활용한 정렬 알고리즘이다.
    - 존 폰 노이만이 고안했다.
  - 과정
    - 정렬하고자 하는 배열을 반으로 분할 한다(divide).
    - 나눈 두 개의 배열을 각각 정렬한다(conquer).
    - 정렬 된 두 개의 배열을 합한다.
  - 시간복잡도
    - O(NlgN)이다(N은 배열의 길이).
    - 정렬하고자 하는 배열의 원소가 1개가 될 때까지 분할하는 시간복잡도는 O(N)이다.
    - 예를 들어 [1,2,3,4,5,6,7,8]을 원소가 1개인 배열들로 분할될 때 까지 분할해보면 아래와 같다.
    - `[1,2,3,4], [5,6,7,8]`(1번), `[1,2], [3,4], [5,6], [7,8]`(2번), `[1], [2], [3], [4], [5], [6], [7], [8]`(4번)
    - 즉 한 번 분할될 때 마다 2<sup>k</sup>번 분할 과정이 실행되게 되고, 총 2N-1번 실행되어 시간복잡도는 O(N)이 된다.
    - 분할된 배열들을 정렬하면서 합치는 과정의 시간복잡도는 O(NlgN)이다.
    - 각 단계에서 정렬에 필요한 시간복잡도는 O(N)이고, 이 과정을 lgN번 반복하므로 병합 과정의 시간 복잡도는 O(NlgN)이다.
    - lgN번 반복하는 이유는, 분할이 완료됐을 때 배열의 길이는 항상 1인데, 한 번 병합을 거칠 때 마다 2배씩 커지며, 최종적으로는 N개가 되어야 한다.
    - 즉 2<sup>k</sup> = N를 만족하는 k의 값이 반복 횟수인데, 양변에 log를 취하면 k=lgN이 되므로 lgN번 반복하게 된다.
    - O(N)과 O(NlgN)중 O(NlgN)이 더 크므로 시간복잡도는 O(NlgN)이 된다.
  - 구현

  ```python
  def merge(arr1, arr2):
      i = 0
      j = 0
      n = len(arr1)
      m = len(arr2)
      sorted_arr = []
      while i < n and j < m:
          if arr1[i] < arr2[j]:
              sorted_arr.append(arr1[i])
              i += 1
          else:
              sorted_arr.append(arr2[j])
              j += 1
  
      if i == n:
          sorted_arr += arr2[j:]
  
      if j == m:
          sorted_arr += arr1[i:]
  
      return sorted_arr
  
  
  def merge_sort(arr):
      if (length := len(arr)) < 2:
          return arr
  
      lh = arr[:length // 2]
      rh = arr[length // 2:]
      return merge(merge_sort(lh), merge_sort(rh))
  
  
  arr = [2, 5, 7, 3, 8, 9, 4, 1]
  print(merge_sort(arr))
  
  ```

  - 병합정렬은 대표적인 안정 정렬(Stable Sort)이다.
    - 안정 정렬이란 중복된 값을 입력 순서와 동일하게 정렬하는 정렬을 의미한다.
    - 즉 입력값의 순서가 정렬 후에도 달라지지 않는 정렬 방식을 말한다.

  ```python
  # 예를 들어 아래와 같은 데이터가 있다.
  users = [
      {
          "age":31,
          "name":"John"
      },
      {
          "age":25,
          "name":"Mike"
      },
      {
          "age":25,
          "name":"Paul"
      }
  ]
  
  # 위 데이터를 안정 정렬을 통해 age를 내림차순으로 정렬하면 아래와 같은 결과가 나온다.
  users = [
      {
          "age":31,
          "name":"John"
      },
      {
          "age":25,
          "name":"Mike"
      },
      {
          "age":25,
          "name":"Paul"
      }
  ]
  
  # 그러나 불안정 정렬을 통해 age를 내림차순으로 정렬하면 아래와 같은 결과가 나올 수도 있다.
  # 입력상으로는 Mike가 Paul보다 먼저 왔지만 불완전 정렬을 거치면서 순서가 뒤바뀌었다.
  users = [
      {
          "age":31,
          "name":"John"
      },
      {
          "age":25,
          "name":"Paul"
      },
      {
          "age":25,
          "name":"Mike"
      }
  ]
  ```



- 퀵 정렬(Quick Sort)

  - 거의 모든 정렬 알고리즘보다 빨라서 퀵 정렬이라는 이름이 붙었다.
    - 피벗을 기준으로 좌우를 나누는 특징 때문에 파티션 교환 정렬이라고도 불린다.
    - 병합 정렬과 마찬가지로 분할 정복 알고리즘을 활용한 알고리즘이다.
  - 과정
    - 피봇을 기준으로 배열을 3등분한다(피봇보다 작은 값들, 피봇, 피봇보다 큰 값들).
    - 피벗 값은 분할시에 확정된다(피벗의 왼쪽에는 피벗 보다 작은 값이, 오른쪽에는 피벗 보다 큰 값이 왔으므로 피벗은 더 이상 움직일 필요가 없다).
    - 분할된 배열을 각각 정렬한다.
    - 각각 정렬된 배열을 다시 합친다.
    - 재귀 호출이 한 번 실행될 때 마다 적어도 피벗의 위치는 정해지므로 이 알고리즘이 반드시 끝이 난다는 것이 보장된다.
  - 파티션 방식
    - 파티션 방식(분할 방식)에 따라 호어 방식(Hoare Partition)과 로무토 방식(Lomuto Partition)으로 나뉜다.
    - 호어 방식은 호어가 퀵 정렬을 고안할 때 사용한 방식으로 배열의 중간값을 피봇으로 선택한다.
    - 로무토 방식은 항상 맨 오른쪽의 피벗을 선택하는 방식으로, 호어 파티션보다 구현이 간결하다는 장점이 있다.
    - 두 방식 모두 최악, 최선일 때 시간복잡도는 같지만, 특정 상황에서 호어 방식이 로무토 방식보다 효율적이다.
  - 로무토 방식 구현
    - 배열의 맨 오른쪽 요소를 피벗으로 선택한다.
    - 두 개의 포인터(left, right)를 배열의 시작점에 위치시키고, right 포인터를 먼저 전진시키면서 pivot보다 작은 값이 나오면 left 포인터가 가리키는 값과 스왑한다.
    - 스왑이 일어났을 경우에만 left 포인터를 전진시킨다.
    - right 포인터가 끝(pivot이 가리키는 요소의 바로 앞 요소)에 도달하면 left 포인터와 pivot의 값을 스왑하고 분할을 종료한다.

  ```python
  # 로무토 방식
  def lomuto_partition(arr, low, high):
      # pivot으로 배열의 가장 오른쪽 값을 선택
      pivot = arr[high]
      left = low
      for right in range(low, high):
          if arr[right] < pivot:
              arr[right], arr[left] = arr[left], arr[right]
              left += 1
  
      arr[left], arr[high] = arr[high], arr[left]
  
      # pivot을 반환한다.
      return left
  
  
  def quick_sort(arr, low, high):
      if low < high:
          pivot = lomuto_partition(arr, low, high)
          quick_sort(arr, low, pivot - 1)
          quick_sort(arr, pivot + 1, high)
  
  
  lst = [2, 4, 5, 2, 1, 4, 6, 7, 2, 9]
  quick_sort(lst, 0, len(lst)-1)
  ```

  - 호어 방식 구현
    - 배열의 맨 왼쪽 요소를 피벗으로 선택한다.
    - 먼저 left 포인터를 피벗 다음에서 출발시켜 피벗보다 큰 값을 찾을 때 까지 오른쪽으로 이동시킨다.
    - 다음으로 right 포인터를 배열의 마지막 값에서 출발하여 피벗보다 작은 값을 찾을 때 까지 왼쪽으로 이동시킨다.
    - 두 포인터가 가리키는 값을 서로 바꾼다.
    - 이 과정을 left 포인터가 right 포인터보다 작거나 같을 때 까지, 즉 두 포인터가 교차할 때 까지 반복한다.
    - 두 포인터가 교차하면 pivot과 right 포인터가 가리키는 값을 교체하고 종료한다.

  ```python
  # 호어 방식
  def hoare_partition(arr, left, right):
      i = left
      j = right
      pivot = arr[left]
      while i <= j:
          while i <= j and arr[i] <= pivot:
              i += 1
          while i <= j and arr[j] >= pivot:
              j -= 1
          if i < j:
              arr[i], arr[j] = arr[j], arr[i]
      arr[left], arr[j] = arr[j], arr[left]
      return j
  
  
  def hoare_quick_sort(arr, low, high):
      if low < high:
          pivot = hoare_partition(arr, low, high)
          hoare_quick_sort(arr, low, pivot - 1)
          hoare_quick_sort(arr, pivot + 1, high)
  
  
  lst = [2, 4, 5, 2, 1, 4, 6, 7, 2, 9]
  hoare_quick_sort(lst, 0, len(lst) - 1)
  ```

  - 평균 시간복잡도
    - 평균적으로 O(NlgN)이다.
    - 피벗의 위치를 찾는 데(partition 함수) 매 단계마다 대략 O(N) 만큼의 시간이 필요하다.
    - 예를 들어 길이가 8인 배열 [4,3,2,6,7,5,1,8]이 있을 때, pivot이 들어갈 위치를 찾기 위해 대략 O(N)의 시간이 걸린다.
    - 또한 위 배열에서 pivot을 찾아 [3,2,1], [4], [6,7,5,8]과 같이 분할되었을 경우 pivot보다 작은 값을 담은 배열의 pivot을 찾는데 대략 O(N/2), pivot보다 큰 값을 담은 배열의 pivot을 찾는 데 대략 O(N/2)의 시간이 필요하다.
    - N/2+N/2=N이므로 다음 단계도 O(N)의 시간이 걸리는 셈이다.
    - 결국 각 단계마다 O(N)의 시간이 걸린다.
    - 단계가 증가할 수록 확정되는 pivot의 개수는 1, 2, 4, 8과 같이 지수적으로 증가하므로 이 과정은 lgN번 반복한다.
    - 따라서 전체 시간 복잡도는 N번 수행해야 하는 작업이 lgN번 반복되므로 O(NlgN)이다.
  - 최악의 경우의 시간복잡도
    - 만약 이미 정렬된 배열을 정렬하려 할 때, pivot값은 호어 방식의 경우 맨 왼쪽의 값으로 고정된다(로토무의 경우 맨 오른쪽 값으로 고정).
    - 따라서 파티셔닝이 전혀 이루어지지 않으므로 pivot을 찾는 단계가 lgN번이 아닌 N번 수행된다.
    - 따라서 시간복잡도는 O(N<sup>2</sup>)이 된다.
  - 특징
    - 다른 O(NlgN) 알고리즘들 보다는 빠른 속도를 보여주는데 매 사이클마다 적어도 1개의 원소(pivot)는 정렬되므로 정렬 대상이 줄어들기 때문이다.
    - 또한 다루는 데이터들의 지역성이 높아 cache hit rate이 높다.
    - 제자리 정렬이지만 불안정 정렬이다.





# 그리디 알고리즘

- 그리디 알고리즘(Greedy Algorithm)
  - 글로벌 최적을 찾기 위해 각 단계에서 로컬 최적의 선택을 하는 휴리스틱 문제 해결 알고리즘.
  - 그리디 알고리즘은 최적화 문제를 대상으로 한다.
    - 최적해를 찾을 수 있으면 그것을 목표로 삼는다.
    - 최적해를 찾기 어려운 경우에는 주어신 시간 내에 그런대로 괜찮은 해를 찾는 것을 목표로 삼는다.



- 그리디 알고리즘의 조건
  - 탐욕 선택 속성(Greedy Choice Property)을 갖고 있는 최적 부분 구조(Optimal Substructure)인 문제들이다.
    - 이 조건을 만족해야만 최적해를 찾을 수 있다.
    - 이 조건을 만족하지 않더라도 정답을 근사하게 찾는 용도로 활용할 수 있으며, 대부분의 경우 계산 속도가 빠르므로 매우 실용적이다.
  - 탐욕 선택 속성
    - 앞의 선택이 이후 선택에 영향을 주지 않는 것.
    - 즉 선택을 다시 고려하지 않는다.
  - 최적 부분 구조
    - 문제의 최적 해결 방법이 부분 문제에 대한 최적 해결 방법으로 구성되는 경우.



- 그리디 알고리즘 풀이 전략
  - 풀이 흐름
    - 관찰을 통해 탐색 범위를 줄이는 방법을 고안한다.
    - 탐색 범위를 줄여도 올바른 결과를 얻을 수 있다는 사실을 증명한다.
  - 정말 그리디 알고리즘으로 풀 수 있다는 확신이 없다면 그리디로 풀어선 안 된다.



- 동전 문제 풀기

  - 그리디 알고리즘으로 풀 수 있는 문제 중에서 가장 유명한 문제가 동전문제이다.
    - 여러 종류의 동전이 있을 때 최소한의 동전을 사용해서 값을 지불하는 문제이다.

  - 문제
    - 500, 100, 50, 10 네 종류의 동전이 있을 때, 1780원을 최소한의 동전으로 만들려고 한다.
    - 500원 짜리 3개, 100원 짜리 2개, 50원 짜리 1개, 10원 짜리 3개를 사용하면 최소 개수로 1780원을 만들 수 있다. 
  - 증명
    - 동전을 최소한으로 사용하면서 원하는 가격을 맞추기 위해서는 10원과 100원짜리 동전은 4개 이하, 50원 짜리 동전은 1개 이하로 사용해야한다.
    - 10원 짜리를 5개를 사용할 경우 50원 짜리 하나를 사용하는게 낫고, 100원짜리 5개를 사용할 경우 500원 짜리 하나를 사용하는 것이 나으며, 50원 짜리 2개를 사용할 경우 100원짜리 하나를 사용하는 것이 낫기 때문이다.
    - 동전을 최소한으로 사용하면서 원하는 가격을 맞추기 위해서는 500원 동전을 최대한 많이 사용해야한다. 
    - 10원짜리의 최대 개수인 4, 100원짜리 최대 개수인 4, 50원 짜리 최대 개수인 1을 모두 더해 봐야 490원이 한계이므로, 500원짜리 동전을 사용할 수 있음에도 사용하지 않을 경우 각 동전의 최대 상한보다 더 많은 양을 사용해야만 한다.
    - 500원을 최대한 사용해야하는 이유와 동일한 이유가 100, 50, 10원에도 적용된다.





## 탐욕 알고리즘 분석 기법

> https://gazelle-and-cs.tistory.com/59
>
> https://web.stanford.edu/class/archive/cs/cs161/cs161.1138/handouts/120%20Guide%20to%20Greedy%20Algorithms.pdf
>
> http://www.cs.cornell.edu/courses/cs482/2007su/ahead.pdf



- 탐욕 알고리즘의 분석
  - "현재로써 최선인 선택을 취한다."는 매우 단순한 원리를 가지고 있지만 탐욕 알고리즘으로 최적해를 구할 수 있다는 것을 증명하는 것은 매우 까다롭다.
    - 직관적으로 그럴 것 같은 것과 정말 그렇다는 것을 증명하는 것은 매우 다르다.
  - 분석이 까다로운 탐욕 알고리즘을 분석할 수 있는 기법들이 몇 가지 있는데 대표적으로 아래와 같은 것들이 있다.
    - Greedy stays ahead
    - Certificate argument
    - Exchange argument

  - Greedy 알고리즘을 사용하여 다양한 문제의 최적해를 구할 수 있다는 것을 각 기법으로 확인해 볼 것이다.



- Interval Scheduling 문제
  - 여러 작업들의 시작 시간과 종료 시간이 주어질 때, 기계가 처리할 수 있는 작업의 최대 개수를 구하는 문제이다.

    - 여러 작업을 처리해야하는 상황에서, 작업을 처리할 수 있는 기계는 하나 뿐이다.
    - 기계는 한 번에 하나의 작업만을 처리할 수 있지만, 한 작업이 끝남과 동시에 다른 작업을 시작할 수 있다.

    - 백준의 [회의실 배정](https://www.acmicpc.net/problem/1931) 문제도 이와 동일한 문제이다.

  - Interval scheduling 문제를 풀기 위한 greedy 알고리즘
    - 모든 작업을 종료 시각의 오름차순으로 정렬한 후 각 작업을 순서대로 고려하면서 기계에 할당할 수 있으면 넣고 그렇지 않으면 버린다.
    - 만일 작업의 종료 시각이 같다면 작업의 시작 시간을 기준으로 정렬한다.



### "Greedy stays ahead" arguments

- 방식
  - 최적해를 출력하는 가상의 알고리즘이 있다고 가정한다. 
  - 이 가상의 알고리즘이 알고리즘이 매번 선택을 할 때 탐욕 알고리즘이 어떤 선택을 하는지 살펴본다. 
  - 만약 탐욕 알고리즘의 선택이 최적 알고리즘의 선택보다 항상 (특정 기준에 따라) 뒤쳐지지는 않는다면, 전체적으로 봤을 때도 탐욕 알고리즘의 출력이 최적 알고리즘의 출력만큼은 좋다고 볼 수 있다.
  - 따라서 탐욕 알고리즘이 내놓은 답이 최적해가 된다.



- 과정
  - Solution 정의
    - greedy algorithm을 구현한다.
    - 구현한 greedy algorithm을 통해 얻을 수 있는 해들과 임의의 최적의 알고리즘을 통해 구할 수 있는 해들을 가정한다.
  - 척도(measure)를 정의
    - greedy algoritm을 통해 구할 수 있는 해들과, 임의의 최적 알고리즘을 통해 구할 수 있는 해들을 하나씩 비교할 수 있는 척도를 정의한다
    - 두 알고리즘을 통해 구한 해의 개수는 경우에 따라 같을 수도 있고 다를 수도 있다.
  - Greedy stays ahead를 증명
    - greedy로 구한 해가, 최적 알고리즘을 통해 구한 해 보다 위에서 정의한 척도 보다 모든 경우 뒤쳐지지는 않는 다는 것을 증명한다.
  - 최적성(optimality) 증명
    - 위에서 증명한 greedy stays ahead를 기반으로 greedy 알고리즘이 반드시 최적해를 산출해 낸다는 것을 증명한다. 
    - 이는 주로 greedy가 최적해를 구할 수 없다는 것을 가정했을 때 모순이 발생한 다는 것을 확인함으로써 이루어진다.



- Solution 정의
  - 작업들의 집합 T가 있으며, T의 원소들은 시작 시각과 종료 시각에 대한 정보를 가지고 있다.

  - 그리디 알고리즘은 집합 T를 종료 시각을 기준으로 오름차순으로 정렬하여 각 작업을 처음부터 순회하면서, 기계에 할당할 수 있으면 자신의 집합에 넣고, 그렇지 않으면 버린다.
    - 이를 통해 처리한 작업들의 집합을 S := {p<sub>1</sub>, p<sub>2</sub>, ... , p<sub>k</sub>}라 하며, 이는 기계에서 처리된 순서로 정렬되어 있다.
  - 임의의 최적의 알고리즘 역시 나름의 방식으로 집합 T에서 처리할 수 있다고 판단하는 작업들을 자신의 집합에 넣는다.
    - 이를 통해 처리한 작업들의 집합을 O := {q<sub>1</sub>, q<sub>2</sub>, ... , q<sub>k</sub>}라 하며, 이 역시 기계에서 처리되는 순서로 정렬되어 있다.



- 척도 정의

  - 어떤 작업 p에 대해 이 작업이 시작하는 시각을 s(p), 끝나는 시각을 f(p)라 한다.
    - 탐욕 알고리즘을 통해 얻은 집합 S의 원소들은 s(p<sub>1</sub>) <= f(p<sub>1</sub>) <= s(p<sub>2</sub>) <= f(p<sub>2</sub>) <= ... <= s(p<sub>k</sub>) <= f(p<sub>k</sub>)를 만족한다.
    - 임의의 최적의 알고리즘을 통해 O의 원소들도 s(p<sub>1</sub>) <= f(p<sub>1</sub>) <= s(p<sub>2</sub>) <= f(p<sub>2</sub>) <= ... <= s(p<sub>k</sub>) <= f(p<sub>k</sub>)를 만족한다.
    - 두 집합의 개별 원소들을 작업 시작 시각과 종료 시각으로 비교할 것이다.
  - 증명하고자 하는 명제는 k >= l 이라는 것이다.
    - 만약 탐욕 알고리즘을 통해 처리한 작업들의 개수가, 최적해를 통해 처리한 작업의 개수보다 크거나 같다면, 탐욕 알고리즘을 통해 최적해를 구할 수 있다는 것이다.
    - 즉, S 집합에 속한 작업의 개수 k가, O집합에 속한 작업의 개수 l보다 크다면, 탐욕 알고리즘을 통해 최적해를 구할 수 있음이 증명된다.



- Greedy stays ahead 증명

  - 집합 S와 집합 O 내에 있는 개별 원소들을 하나씩 비교하면서 개별 원소들에서 greedy로 구한 해가 최적 알고리즘으로 구한 해 보다 특정 척도(종료 시각)에서 뒤쳐지지 않는지를 확인할 것이다.
  - 정리: 1<= i <=m을 만족하는 모든 i에 대해 f(p<sub>i</sub>) <= f(q<sub>i</sub>)가 성립한다.
    - 이는 너무 분명한 사실로, 탐욕 알고리즘은 선택 가능한 작업들 중 종료 시간이 가장 빠른 작업을 선택한다.
    - 따라서 같은 순서(i번째)에 있는 S에 속한 작업 p<sub>i</sub>와 q<sub>i</sub>중 p<sub>i</sub>가 더 빠르거나, 최소한 q<sub>i</sub>와 같을 것이다. 
    - 즉, S에 속한 모든 i번째 작업의 종료 시각은 O에 속한 모든 i번째 작업의 종료 시각보다 빠르다.

  - Base case
    - 만약 i=1이라면 탐욕 알고리즘은 주어진 작업들 중 가장 빨리 끝나는 작업을 할당할 것이므로, f(p<sub>1</sub>) <= f(q<sub>1</sub>)을 만족할 것이다.
    - S는 작업을 처리하는 순서대로 정렬되어 있으므로 f(p1)은 모든 작업들 중 종료 시간이 가장 빠른 값일 수 밖에 없다. 
    - 따라서 f(p<sub>1</sub>)은 f(q<sub>1</sub>)보다 작거나 같을 수 밖에 없다.



- 최적성 증명
  - 반박될 가정: S가 최적해의 집합이 아니라면, S에 속한 작업의 개수 k는 O에 속한 작업의 개수 l보다 작을 것이다.
    - 만약 이 가정이 참이라면, S의 마지막 작업이 k번째 작업이기 때문에, O에는 k+1번째 작업이 있다는 말이 된다.
  - O의 k+1번째 작업인 q<sub>k+1</sub>은 q<sub>k</sub> 이후에 실행되어야 한다.
    - 그런데, 우리는 "모든 i = 1, ..., l에 대해 f(p<sub>i</sub>) <= f(q<sub>i</sub>)를 만족한다."는 사실을 알고 있다.
    - 이를 통해 S의 k번째 작업인 p<sub>k</sub>가 O의 k번째 작업인 q<sub>k</sub>보다 종료 시각이 빠르다는 거나 같다는 알 수 있다.
    - 따라서 f(p<sub>k</sub> ) <= f(q<sub>k</sub>) <= s(q<sub>k+1</sub>)와 같은 상황이 된다.
    - 이는 O의 k+1번째 작업이 S에 추가될 수 있음을 의미하는데, 이는 모순이다.
    - 따라서 S는 O 만큼의 작업의 개수를 가진다는 것을 알 수 있다.





### Certificate argument

- 방식
  - 알고리즘이 답과 함께 그것이 최적해라는 증거를 함께 출력하도록 하는 방식이다.



- 문제 및 그리디를 통한 해결
  - Interval schduling 문제의 변형
    - 여러 작업들의 시작 시간과 종료 시간이 주어질 때, 모든 작업을 처리하기 위한 기계의 개수를 구하는 문제.
    - 이전과는 달리 하나 이상의 기계를 사용할 수 있다.
    - 모든 작업을 처리해야하는데, 최소한의 기계만을 사용하여 모든 작업을 처리하고자한다.
    - 마찬가지로 하나의 기계는 하나의 작업만을 처리할 수 있으며, 한 작업이 끝남과 동시에 다른 작업을 처리할 수 있다.
    - 백준의 [강의실 배정](https://www.acmicpc.net/problem/11000)이 이와 유사한 문제이다.
  - 그리디를 활용한 해결 방법
    - 매 시각 t에 시작하는 작업이 있을 때, 지금까지 구매한 기계에 할당할 수 있으면 그것들 중 아무 것에 할당한다.
    - 현재까지 구매한 기계만으로 불가능하다면, 기계를 하나 더 구매하고 거기에 할당한 다음 t* ← t를 기억한다.



- 증명과정
  - 작업의 집합 P가 주어진다.
    - 각 작업 p ∈ P는 s(p)에 시작하고 f(p)에 끝난다.
    - 수행 시간이 겹치는 작업 두 개를 한 기계에 할당할 수 없다.
    - 그러나 한 작업이 끝날 때 바로 다음 작업을 실행할 수는 있다.
    - 모든 작업의 수행 시간을 [s(p), f(p))로 표현하여, 이러한 조건을 나타낼 수 있다.
  - 이 문제의 목적은 서로 겹치는 작업이 없는 P의 부분 집합들 중 원소의 개수가 가장 작은 부분 집합을 찾아내는 것이다.
    - 서로 겹치는 작업이 없는 P의 부분 집합들의 집합을 S라 한다.
    - 즉 임의의 서로 다른 두 작업 p, p' ∈ S에 대해, $$[s(p), f(p))∩[s(p'), f(p')) = \empty$$이다.
    - 따라서 우리의 목표는 S 중 그 크기가 가장 작은 것을 찾는 것이다.
  - 모든 작업을 소화하기 위해서 최소한 몇 대의 기계가 필요한가?
    - 모든 작업을 수행해야 하므로 어떤 시각에 동시에 수행되는 작업의 개수보다 적지 않아야한다.
    - 즉 임의의 시각 t에 수행되는 작업의 집합을 $$P_t := {p \in P : t \in [s(p),f(p))}$$라고 할 때, 반드시 아래의 정리를 만족해야한다.
  - 정리
    - 모든 작업을 할당하기 위해서는 임의의 시각 t에 대해 $$|P_t|$$ 이상의 기계가 필요하다.
    - 즉, S가 올바른 답이라면 반드시 $$|S|\ge max_t |P_t|$$를 만족한다.
    - 위 정리를 통해 아래와 같은 정리를 이끌어낼 수 있다.
    - 만약 알고리즘이 올바른 답 S와 함께 $$|S|=|P_{t*}|$$인 어떤 시각 t*를 찾는다면, S는 최적해이다.
  - 증명
    - 문제의 최적해를 O라 하자, 이 답은 올바른 답이므로 정리에 의해 $$|O| \ge max_t |P_t| \ge |P_{t*}| = |S|$$를 만족한다.
    - O는 최적해인데, S가 그것보다 크기가 더 작거나 같으므로 S도 최적해이다.
    - 즉, 알고리즘이 S와 함께 $$|S|=|P_{t*}|$$를 만족하는 어떤 시각 t*를 증거로 제출하면, 알고리즘이 최적의 답을 반환한다는 것을 보일 수 있다.



- Interval scheduling 문제에 적용

  - 이전에 살펴본 interval scheduling 문제에 적용하면 아래와 같은 과정을 거친다.
  - 작업들의 집합 P가 주어질 때, OPT(P)는 P의 작업들 중 서로 겹치는 작업이 없는 부분 집합의 최대 원소 개수를 나타낸다.
    - 서로 겹치는 작업이 없는 부분 집합을 S라 한다.
  - OPT(P)의 최솟값은 무엇인가?
    - OPT(P)가 적어도 k보다는 크거가 같다는 사실(즉 k가 OPT(P)의 최솟값이라는 사실)은 S 중 크기가 k 이상인 것이 있음을 보이면 된다.
    - 즉 $$OPT(P) \ge k$$임을 보이기 위해서는 원소의 개수가 k인 S가 있음을 보이면 된다.
    - S가 P의 서로 겹치는 작업이 없는 부분집합일 때, $$OPT(P) \ge |S|$$이 성립한다.
    - 즉 원소의 개수가 k인 S가 있다면 OPT(P)의 최솟값이 k라는 것을 certificate한다.
  - OPT(P)의 최댓값은 무엇인가?
    - Partition이란 P의 원소들 중 서로 시간이 겹치는 작업들을 모아놓은 부분집합이다.
    - 따라서 모든 partition을 합하면 전체 집합(P)가 된다.
    - 하나의 parition 내에서는 작업들 간에 서로 시간이 겹치기 때문에 답을 구하기 위해서는 partition마다 작업을 하나씩만 뽑아야한다(한 partition에서 둘 이상의 작업을 뽑을 경우 서로 겹치는 작업이 생기게 된다).
    - 예를 들어 아래 표에서 작업 시간이 겹치는 p1과 p2를 하나의 partition으로, p3를 하나의 파티션으로, p4, p5를 하나의 파티션으로 묶을 수 있다.
    - 혹은 [p1], [p2,p3], [p4,p5]와 같이 묶을 수도 있다.
    - 만약 $$P_1, ..., P_k$$가 P의 partition이고, 모든 $$i=1, ..., k$$에 대하여 $$P_i$$의 모든 작업들의 쌍이 서로 시간이 겹친다면, $$OPT(P) \le k$$이다.
    - 즉 OPT(P)의 최댓값은 P의 partition의 개수이다.

  |      | 0    | 1    | 2    | 3    | 4    | 5    | 6    | 7    |
  | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- |
  | p1   | -    | -    | -    |      |      |      |      |      |
  | p2   |      | -    | -    | -    |      |      |      |      |
  | p3   |      |      |      | -    | -    | -    |      |      |
  | p4   |      |      |      |      |      |      | -    | -    |
  | p5   |      |      |      |      |      |      |      | -    |

  - 증명
    - O를 최적해라고 했을 때, 하나의 partition안에 있는 작업들과 O에 있는 작업들 중 겹치는 작업은 최대 1개이다.
    - 한 partition 안에서는 모든 작업들의 시간이 겹치기 때문에, 최적해만 모여있는 O에서는 최대 1개까지만 겹칠 수 있다.
    - 따라서 k개의 partition들 내부의 작업들과  O 내부의 작업들 중 겹치는 것을 모두 합해봐야 k개가 최대이다.
    - $$OPT(P) = O = \sum^k_{i=1} P_i \cap O \le k$$
    - 이렇게 구해진 OPT(P)의 최솟값과, 최댓값을 함께 제시하여 OPT(P)가 최적해임을 보장(certificate)할 수 있다.



### Exchange Argument

> https://web.stanford.edu/class/archive/cs/cs161/cs161.1138/handouts/120%20Guide%20to%20Greedy%20Algorithms.pdf

- Exchange Argument
  - Greedy algorithm의 최적성을 증명하기 위한 강력하고 다재다능한 기법이다.
  - 어떤 최적해를 반복적으로 greedy algorithm이 반환한 해로 변환해준다.
    - 이를 통해 greedy algorithm이 반환한 해가 최적해임을 증명한다.



- 일반적인 과정

  - Solution을 정의한다.
    - Greedy algorithm과 이것이 반환할 해를 정의한다.
    - Exchange Argument는 greedy algorithm이 반환한 해인 X를 최적해인 X*와 비교하는 방식으로 동작하기에 해를 명확하게 정의하는 것이 중요하다.
  - 해들을 비교한다.
    - 만약 X가 X*와 같지 않다면, 어디가 같지 않은지를 보여야한다.
    - 예를 들어 X의 일부가 X*에는 포함되지 않거나, X에 포함되어 있는 2개의 원소가  X\*에도 포함은 되어 있지만, 포함된 순서가 다를 수 있다.

  - Piece들을 교환(exchange)한다.
    - X*의 일부(piece)가 어떻게 X의 일부로 변환되는지를 보여야한다.
    - 일반적으로, 이전 단계에서 정의한 piece를 사용한다.
    - 이렇게 함으로써, X*의 cost를 증가시키거나 감소시키지 않고 최적해를 구할 수 있다.
  - 반복한다.
    - 최적해의 quaility에 영향을 주지 않으면서 교환을 수행함으로써 X와 X*의 차이를 줄이는 작업을 반복적으로 수행하여 X\*를 X로 변환한다.
    - 이를 통해 X는 최적해가 된다.
    - 즉 아래 과정을 통해서 최적해 O가 greedy를 통한 해 G로 순차적으로 변환된다.
    - O(최적) → O'(최적) → O''(최적) → O'''(최적) → ... → A(최적)



- 주의사항
  - X != X*라도 X가 최적해일 수 있다.
    - 한 문제에 대한 여러 개의 최적해가 있을 수 있기 때문이다.
    - 따라서 X!=X*라도 모순이 발생하지 않을 수 있다.
    - 만약 최적해가 단 1개뿐이라면, 모순이 발생할 것이다.
  - 한 번만 변환한다고 X*가 X가 되는 것은 아니다.
    - 한 번의 변환은 X*를 X가 서로 더 유사해지게는 하지만, X와 X\*가 동일한 상태가 된다는 것을 보장하지는 않는다.
    - 따라서 반복적으로 변환을 수행해야 하며, 모든 반복에는 해당 절차가 반복되어야하는 정당한 이유가 있어야한다.
  - X*를 X로 변환해야하는 이유가 명확해야한다.



- Minimize Lateness 문제
  - 수행해야 할 여러 개의 작업이 주어진다.
    - 각각의 작업에는 수행하는 데 걸리는 시간과 서로 다른 마감 기한이 있다.
  - 작업을 수행할 수 있는 기계는 1개 뿐이고, 기계는 여러 개의 작업을 동시에 진행할 수 없을 때, L의 최솟값을 구하라.
    - lateness는 한 작업이 작업 기한을 넘긴 정도중 최대값을 의미한다.
    - 예를 들어 마감 기한이 3인 A 작업을 4에 마쳤다면 lateness는 1이 된다.
    - L은 여러 작업의 lateness 중 최댓값을 의미한다.
    - 예를 들어 마감 기한이 3인 A 작업을 4에 마치고, 마감 기한이 4인 작업 B를 6에 마쳤다면 L은 작업 B가 기한을 넘긴 정도인 2가된다.
    - L의 값 중 최솟값을 찾아야한다.



- Exchange argument 적용

  - 문제 형식화하기

    - $$t_j$$는 작업 j를 완료하는 데 걸리는 시간이다.
    - $$d_j$$는 작업 j의 마감 기한이다.
    - $$s_j$$는 작업 j의 작업 시작 시각이다(이는 알고리즘에 의해 정해진다).
    - $$f_j=s_j+t_j$$는 작업 j가 끝나는 시각이다.
    - $$l_j$$는 작업 j의 lateness이다.

    $$
    l_j = \begin{cases}
    0 & \mbox{if}\ f_j \le d_j \\
    f_j - d_j, & \mbox{if }\ f_j > d_j
    \end{cases}
    $$

    - $$L=max_jl_j$$은 lateness 중 최댓값이다.

  - 목표는  L이 최솟값이 되는 일정을 찾는 것이다.

    - 이를 위해 여러 가지 정렬 방식이 있을 수 있다.
    - 작업 기간($$t_j$$)을 기준으로 정렬
    - 마감 기한($$d_j$$)을 기준으로 정렬
    - $$d_j-t_j$$ 기준으로 정렬
    - 이들 중 마감 기한을 기준으로 정렬했을 때만 최적해를 얻을 수 있다.

  - 증명 방식

    - 최적해 O와 greedy에 의해(마감 기한을 기준으로 정렬) 얻어진 해 G가 있다.
    - O와 G가 같지 않다고 가정한다. 
    - 새로운 해 O'를 얻기 위해 O를 수정할 수 있다.
    - O'는 O보다 나쁘지 않으며, G와 더 유사하다는 두 가지 조건을 만족해야한다.

  - 유휴시간

    - 이전 문제들과는 달리 시작 시각이 정해진 것이 아니다.
    
    - 시작 시각이 정해진 것이 아니기에 작업과 작업 사이에 아무 작업도 두지 않는 유휴시간은 두지 않는다.
    
  - Inversion

    - 이 문제에서 inversion이란 마감 기한이 더 느린 작업이 마감 기한이 더 빠른 작업보다 앞에 배치되는 상황을 의미한다.
    - 어떤 최적해 O에 inversion이 있다고 가정해보자.
    - 예를 들어 마감 기한이 1,2,3,4,5인 작업들이 있을 때 최적해 O에서 [1,2,3,5,4] 순으로 일정이 정렬되어 있다고 가정해보자.
    - 마감 기한이 더 느린 5가 4 앞에 왔으므로 inversion이 있는데, 이 두 작업을 swap하면 [1,2,3,4,5]가 되고, 이를 O'라고 해보자.
    - 우리가 보이고자 하는 것은 O를 O'처럼 변경하면 O보다 나빠지지 않으면서, G와 더 유사해짐을 보여야한다.
  
  - O를 O'처럼 변경하면 G와 유사해지는가?
  
    - G는 greedy에 의해 얻어진 해이므로 inversion이 있을 수 없다. 
    - 따라서 최적해 O에서 inversion을 제거하는 것은 G에 가까워지는 과정이라고 볼 수 있다.
  
  - O를 O'처럼 변경하면 O보다 나빠지지는 않는가?
  
    - 이 문제에서 O'가 O보다 나쁘지 않다는 것의 의미는 L이 더 커지지 않는다는 것을 의미한다.
    - 유휴시간이 없으므로, O가 O'가 된다고 L이 더 커지지는 않기에 나빠지지 않는다.
  
  - 따라서 O를 G로 변경하는 것이 가능하므로 G는 최적해라고 볼 수 있다.



- Interval scheduling 문제에 적용
  - 기호 정리
    - G = p<sub>1</sub>, ..., p<sub>k</sub>를 greedy algorithm에 의해 선택된 작업들이라고 가정한다.
    - O = q<sub>1</sub>, ..., q<sub>l</sub>를 최적해를 산출하는 algorithm에 의해 선택된 작업들이라고 가정한다.
    - 두 집합 모두 종료 시각을 기준으로 정렬되어 있다.
  - O를 A로 변환하면서 A가 최소한 O가 가지고 있는 만큼의 작업을 가지고 있다는 것을 보이는 것이 목표이다.
  - p<sub>i</sub>, q<sub>i</sub>가 서로 다른 최초의 element들이라고 가정해보자.
    - 즉, p<sub>i-1</sub>, q<sub>i-1</sub>까지는 서로 같았다.
    - p<sub>i-1</sub>, q<sub>i-1</sub>까지는 서로 같았기 때문에,  p<sub>i</sub>와 q<sub>i</sub>는 q<sub>1</sub>, ..., q<sub>i</sub>까지 중 어떤 것과도 작업 시간이 겹치지 않는다.
    - 또한 G는 가장 빨리 끝나는 작업들을 선택하므로,  p<sub>i</sub>의 종료 시각은 q<sub>i</sub>의 종료 시각보다 빠르거나 같다.
    - 따라서,  p<sub>i</sub>는 q<sub>i+1</sub>과 작업 시간이 겹치지 않는다.
    - 따라서 O에서 q<sub>i</sub>를  p<sub>i</sub>로 바꾼다 하더라도 O 집합 내에서 작업들은 여전히 겹치지 않으므로,  q<sub>i</sub>를  p<sub>i</sub>로 교체가 가능하다.
    - 이를 O가 G가 될 때까지 반복하면 된다.
  - O는 G 보다 많은 원소를 가질 수 없다.
    - 위 방식을 반복하면 O의 모든 원소를 G로 변환할 수 있다.
    - 만약 O가 G에 없는 작업을 가지고 있다면, 이 원소는 O에 있는 어떤 작업과도 겹쳐선 안된다.
    - 그런데 이는 G에 있는 어떤 작업과도 겹칠 수 없으므로 만약 그런 작업이 있었다면 G에도 포함되었을 것이다.
    - 그러므로 O는 G보다 많은 원소를 가질 수 없다.







